{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed520cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred for PMID: 12882011. Retrying... Attempt 1\n",
      "Chunk 1 completed. Time taken: 469.55 seconds\n",
      "Error occurred for PMID: 26700935. Retrying... Attempt 1\n",
      "Chunk 2 completed. Time taken: 463.91 seconds\n",
      "Chunk 3 completed. Time taken: 398.93 seconds\n",
      "Chunk 4 completed. Time taken: 426.78 seconds\n",
      "Error occurred for PMID: 34023712. Retrying... Attempt 1\n",
      "Chunk 5 completed. Time taken: 463.44 seconds\n",
      "Error occurred for PMID: 33616125. Retrying... Attempt 1\n",
      "Error occurred for PMID: 1985602. Retrying... Attempt 1\n",
      "Chunk 6 completed. Time taken: 500.69 seconds\n",
      "Error occurred for PMID: 29804138. Retrying... Attempt 1\n",
      "Chunk 7 completed. Time taken: 455.03 seconds\n",
      "Error occurred for PMID: 29506959. Retrying... Attempt 1\n",
      "Chunk 8 completed. Time taken: 393.55 seconds\n",
      "Chunk 9 completed. Time taken: 94.00 seconds\n",
      "Total time taken: 3665.87 seconds\n"
     ]
    }
   ],
   "source": [
    "### extracts keywords for metabolites \n",
    "\n",
    "\n",
    "import time\n",
    "from Bio import Entrez, Medline\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from math import ceil\n",
    "from http.client import IncompleteRead\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "def search_metabolite_functions(query):\n",
    "    Entrez.email = \"soumyapooja39@gmail.com\"\n", ## enter your mail_id 
    "    attempt = 1\n",
    "    while attempt <= 3:\n",
    "        try:\n",
    "            handle = Entrez.esearch(db='pubmed', sort='relevance', retmax='10', term=query)\n",
    "            result = Entrez.read(handle)\n",
    "            handle.close()\n",
    "            return result[\"IdList\"]\n",
    "        except RuntimeError as e:\n",
    "            print(f\"RuntimeError occurred. Retrying... Attempt {attempt}\")\n",
    "            attempt += 1\n",
    "            if attempt > 3:\n",
    "                print(f\"Maximum retries reached. Skipping.\")\n",
    "                return []\n",
    "\n",
    "def fetch_article_text(id_list):\n",
    "    Entrez.email = \"soumyapooja39@gmail.com\"\n", ## enter your mail_id
    "    articles = []\n",
    "    for pmid in id_list:\n",
    "        attempt = 1\n",
    "        while attempt <= 3:\n",
    "            try:\n",
    "                handle = Entrez.efetch(db='pubmed', rettype='medline', retmode='text', id=pmid)\n",
    "                record = Medline.read(handle)\n",
    "                article_text = record.get('AB', 'N/A')\n",
    "                if article_text != 'N/A':\n",
    "                    article_text += f\" [PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/{pmid}' target='_blank'>{pmid}</a>]\"\n",
    "                articles.append(article_text)\n",
    "                handle.close()\n",
    "                break\n",
    "            except (IncompleteRead, HTTPError) as e:\n",
    "                print(f\"Error occurred for PMID: {pmid}. Retrying... Attempt {attempt}\")\n",
    "                attempt += 1\n",
    "                if attempt > 3:\n",
    "                    print(f\"Maximum retries reached for PMID: {pmid}. Skipping.\")\n",
    "    return articles\n",
    "\n",
    "def read_metabolite_names_from_excel(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    metabolite_names = df['Metabolite'].tolist()\n",
    "    return metabolite_names\n",
    "\n",
    "def main():\n",
    "    # Read metabolite names from an Excel file\n",
    "    metabolite_names = read_metabolite_names_from_excel(\"F:/NISER internship/text-mining for everyone's common metabolites/Sristi_di(500).xlsx\")\n",
    "\n",
    "    # Set the chunk size\n",
    "    chunk_size = 60\n",
    "\n",
    "    # Calculate the number of chunks required\n",
    "    num_chunks = ceil(len(metabolite_names) / chunk_size)\n",
    "\n",
    "    # Create a new Excel file for results\n",
    "    wb = openpyxl.Workbook()\n",
    "    sheet = wb.active\n",
    "    sheet.title = 'Metabolite Functions'\n",
    "    sheet.append(['Metabolite', 'Article Text'])\n",
    "\n",
    "    total_start_time = time.time()  # Track total time taken\n",
    "\n",
    "    # Iterate over each chunk of metabolite names\n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = (i + 1) * chunk_size\n",
    "\n",
    "        # Get the current chunk of metabolite names\n",
    "        metabolite_chunk = metabolite_names[start_idx:end_idx]\n",
    "\n",
    "        chunk_start_time = time.time()  # Track time taken for each chunk\n",
    "\n",
    "        # Iterate over each metabolite name in the current chunk\n",
    "        for metabolite_name in metabolite_chunk:\n",
    "            query = f'{metabolite_name}'\n",
    "            # Search for the query\n",
    "            id_list = search_metabolite_functions(query)\n",
    "\n",
    "            # Fetch article text\n",
    "            articles = fetch_article_text(id_list)\n",
    "\n",
    "            # Save the metabolite name and article text in the Excel file\n",
    "            for article in articles:\n",
    "                sheet.append([metabolite_name, article])\n",
    "\n",
    "        chunk_end_time = time.time()  # Track time taken for each chunk\n",
    "        chunk_time = chunk_end_time - chunk_start_time\n",
    "        print(f\"Chunk {i+1} completed. Time taken: {chunk_time:.2f} seconds\")\n",
    "\n",
    "    total_end_time = time.time()  # Track total time taken\n",
    "    total_time = total_end_time - total_start_time\n",
    "\n",
    "    # Save the Excel file\n",
    "    wb.save(\"F:/NISER internship/text-mining for everyone's common metabolites/Sristi_di(500).xlsx\")\n",
    "\n",
    "    print(f\"Total time taken: {total_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0050aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### merge articles and extract URL\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(\"F:/NISER internship/text-mining for everyone's common metabolites/Sristi_di(500).xlsx\")\n",
    "\n",
    "# Convert NaN or float values to empty strings\n",
    "df['Article Text'] = df['Article Text'].fillna('').astype(str)\n",
    "\n",
    "# Extract URLs from the 'Article Text' column\n",
    "df['urls'] = df['Article Text'].apply(lambda text: re.findall(r'(https?://\\S+)', str(text)))\n",
    "\n",
    "# Convert the list of URLs to a string\n",
    "df['urls'] = df['urls'].apply(lambda urls: ', '.join(urls))\n",
    "\n",
    "# Group by metabolite and concatenate functions\n",
    "df_merged = df.groupby('Metabolite').agg({'Article Text': ', '.join, 'urls': ', '.join}).reset_index()\n",
    "\n",
    "# Add count of merged rows\n",
    "df_merged['Row Count'] = df.groupby('Metabolite').size().reset_index(name='Count')['Count']\n",
    "\n",
    "# Save the merged data to a new Excel file\n",
    "df_merged.to_excel(\"F:/NISER internship/text-mining for everyone's common metabolites/Sristi_di(500)merged_WKW.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f97e7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### keyword matching\n",
    "## if needs to add more keywords then add in this format in the Keyword section r\"\\bkeyword\\b\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(\"F:/NISER internship/text-mining for everyone's common metabolites/Sristi_di(500)merged_WKW.xlsx\")\n",
    "\n",
    "\n",
    "# List of keywords\n",
    "keywords = [\n",
    "    r\"\\bAdipose\\b\", r\"\\bAdipogenesis\\b\", r\"\\bDifferentiation\\b\", r\"\\bBrowning\\b\", r\"\\bBeiging\\b\",\n",
    "    r\"\\bLipogenesis\\b\", r\"\\bAdipocyte\\b\", r\"\\bObesity\\b\", r\"\\bObese\\b\", r\"\\bLipid accumulation\\b\",\n",
    "    r\"\\bLipolysis\\b\", r\"\\bWAT\\b\", r\"\\bBAT\\b\", r\"\\bFat accumulation\\b\", r\"\\bFat degradation\\b\",\n",
    "    r\"\\bFat deposition\\b\", r\"\\bColitis\\b\", r\"\\bInflammatory Bowel Disease\\b\", r\"\\bIBD\\b\",\n",
    "    r\"\\bInflammation\\b\", r\"\\bInflammatory response\\b\", r\"\\bPro-inflammation\\b\",\n",
    "    r\"\\bAnti-inflammation\\b\", r\"\\bGut barrier integrity\\b\", r\"\\bGut permeability\\b\",\n",
    "    r\"\\bColon cancer\\b\"\n",
    "]\n",
    "\n",
    "# Initialize a new column to store matched keywords\n",
    "df['Matched Keywords'] = \"\"\n",
    "\n",
    "# Iterate over each row\n",
    "for index, row in df.iterrows():\n",
    "    text = str(row['Article Text'])  # Replace 'Your Column Name' with the actual column name in your Excel file\n",
    "    matched_keywords = [re.sub(r\"\\b\", \"\", keyword) for keyword in keywords if re.search(keyword, text, flags=re.IGNORECASE)]\n",
    "    if matched_keywords:\n",
    "        df.at[index, 'Matched Keywords'] = ', '.join(matched_keywords)\n",
    "\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "df.to_excel(\"F:/NISER internship/text-mining for everyone's common metabolites/Sristi_di(500)merged_WKW_updated_file.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2804c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "### code for extraction of PubChem_id\n",
    "### no issues if the chunk size is not 100 the code still works and if requires then can increase as per the run speed of the code \n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Read the Excel file with metabolite names\n",
    "df = pd.read_excel(\"F:/NISER internship/Text-minning/pubchem_id_description/metabolites(7179)(grp-2).xlsx\")  # Replace 'metabolites.xlsx' with your file name\n",
    "\n",
    "# Function to search for PubChem ID\n",
    "def search_pubchem_id(metabolite_name):\n",
    "    url = f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{metabolite_name}/cids/JSON\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if 'IdentifierList' in data:\n",
    "            pubchem_id = data['IdentifierList']['CID'][0]\n",
    "            return pubchem_id\n",
    "    return None\n",
    "\n",
    "# Chunk size for processing metabolite names\n",
    "chunk_size = 100\n",
    "\n",
    "# Get the total number of metabolite names\n",
    "total_names = len(df)\n",
    "\n",
    "# Process metabolite names in chunks\n",
    "for i in range(0, total_names, chunk_size):\n",
    "    # Extract the chunk of metabolite names\n",
    "    chunk = df.iloc[i:i+chunk_size]\n",
    "\n",
    "    # Add a new column 'PubChem ID' and search for PubChem ID for each metabolite name in the chunk\n",
    "    chunk['PubChem ID'] = chunk['Metabolite'].apply(search_pubchem_id)\n",
    "\n",
    "    # Save the updated chunk to an Excel file\n",
    "    output_file = f\"Metabolite_pubchem_ID_withIDs_{i+1}-{i+len(chunk)}.xlsx\"\n",
    "    chunk.to_excel(output_file, index=False)\n",
    "    print(f\"PubChem IDs extracted and saved to {output_file}\")\n",
    "\n",
    "print(\"All metabolite names processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa55b5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
